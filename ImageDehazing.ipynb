{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageDehazing.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM8S9DjPXjTP"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os, shutil\n",
        "from tensorflow.keras.layers import *\n",
        "import glob\n",
        "import random\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yK8BcnuNf5J"
      },
      "source": [
        "## **Clone Repository From Github**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGB6fU7Wfy86",
        "outputId": "4941c06f-1777-4c85-f239-ea3034576145",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! git clone \"https://github.com/tusharsircar95/All-In-One-Image-Dehazing-Tensorflow/\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'All-In-One-Image-Dehazing-Tensorflow' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i97MpbER7dxY"
      },
      "source": [
        "# **Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TVsGGZC7S5F"
      },
      "source": [
        "n_epochs = 10\n",
        "batch_size = 8\n",
        "learning_rate = 1e-4\n",
        "weight_decay = 1e-4"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6jHdkEX7Bvv"
      },
      "source": [
        "# **Network Definition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCEbg0A5X5-b"
      },
      "source": [
        "def haze_net(X):\n",
        "\n",
        "  conv1 = Conv2D(3,1,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(stddev=0.02),\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(X)\n",
        "  conv2 = Conv2D(3,3,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(stddev=0.02),\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(conv1)\n",
        "  concat1 = tf.concat([conv1,conv2],axis=-1)\n",
        "\n",
        "  conv3 = Conv2D(3,5,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(stddev=0.02),\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(concat1)\n",
        "  concat2 = tf.concat([conv2,conv3],axis=-1)\n",
        "\n",
        "  conv4 = Conv2D(3,7,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(stddev=0.02),\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(concat2)\n",
        "  concat3 = tf.concat([conv1,conv2,conv3,conv4],axis=-1)\n",
        "\n",
        "  conv5 = Conv2D(3,3,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(stddev=0.02),\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(concat3)\n",
        "  K = conv5\n",
        "\n",
        "  output = ReLU(max_value=1.0)(tf.math.multiply(K,X) - K + 1.0)\n",
        "  #output = output / 255.0\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "# Experimental Network with Res-Net type connections\n",
        "def haze_res_net(X):\n",
        "\n",
        "  conv1 = Conv2D(3,1,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(),\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(X)\n",
        "  conv2 = Conv2D(3,3,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(),\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(conv1)\n",
        "  add1 = conv1 + conv2\n",
        "\n",
        "  conv3 = Conv2D(3,5,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(),\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(add1)\n",
        "\n",
        "  conv4 = Conv2D(3,7,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(),\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(conv3)\n",
        "  add2 = conv3 + conv4\n",
        "\n",
        "  conv5 = Conv2D(3,3,1,padding=\"SAME\",activation=\"relu\",use_bias=True,kernel_initializer=tf.initializers.random_normal(),\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(add2)\n",
        "  add3 = conv5 + conv1\n",
        "  K = add3\n",
        "\n",
        "  output = ReLU(max_value=1.0)(tf.math.multiply(K,X) - K + 1.0)\n",
        "  #output = output / 255.0\n",
        "\n",
        "  return output\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLewmbK37I29"
      },
      "source": [
        "# **Data Loading & Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIcPe_UCcNn8"
      },
      "source": [
        "def setup_data_paths(orig_images_path,hazy_images_path):\n",
        "\n",
        "  orig_image_paths = glob.glob(orig_images_path + \"/*.jpg\")\n",
        "  n = len(orig_image_paths)\n",
        "  random.shuffle(orig_image_paths)\n",
        "\n",
        "  train_keys = orig_image_paths[:int(0.90*n)]\n",
        "  val_keys = orig_image_paths[int(0.90*n):]\n",
        "\n",
        "\n",
        "  split_dict = {}\n",
        "  for key in train_keys:\n",
        "    split_dict[key] = 'train'\n",
        "  for key in val_keys:\n",
        "    split_dict[key] = 'val'\n",
        "\n",
        "  train_data = []\n",
        "  val_data = []\n",
        "\n",
        "  hazy_image_paths = glob.glob(hazy_images_path + \"/*.jpg\")\n",
        "  for path in hazy_image_paths:\n",
        "    label = path.split('/')[-1]\n",
        "    orig_path = orig_images_path + \"/\" + label.split('_')[0] + '_' + label.split('_')[1] + \".jpg\"\n",
        "    if(split_dict[orig_path] == 'train'):\n",
        "      train_data.append([path,orig_path])\n",
        "    else: val_data.append([path,orig_path])\n",
        "\n",
        "  return train_data, val_data\n",
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKvs6a0jRXXO"
      },
      "source": [
        "def load_image(X):\n",
        "  X = tf.io.read_file(X)\n",
        "  X = tf.image.decode_jpeg(X,channels=3)\n",
        "  X = tf.image.resize(X,(480,640))\n",
        "  X = X / 255.0\n",
        "  return X\n",
        "\n",
        "def showImage(x):\n",
        "  x = np.asarray(x*255,dtype=np.int32)\n",
        "  plt.figure()\n",
        "  plt.imshow(x)\n",
        "  plt.show()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPV56lybcfne"
      },
      "source": [
        "def create_datasets(train_data,val_data,batch_size):\n",
        "\n",
        "  train_ds_hazy = tf.data.Dataset.from_tensor_slices([data[0] for data in train_data]).map(lambda x: load_image(x))\n",
        "  train_ds_orig = tf.data.Dataset.from_tensor_slices([data[1] for data in train_data]).map(lambda x: load_image(x))\n",
        "  train_ds = tf.data.Dataset.zip((train_ds_hazy,train_ds_orig)).shuffle(100).repeat().batch(batch_size)\n",
        "\n",
        "  val_ds_hazy = tf.data.Dataset.from_tensor_slices([data[0] for data in val_data]).map(lambda x: load_image(x))\n",
        "  val_ds_orig = tf.data.Dataset.from_tensor_slices([data[1] for data in val_data]).map(lambda x: load_image(x))\n",
        "  val_ds = tf.data.Dataset.zip((val_ds_hazy,val_ds_orig)).shuffle(100).repeat().batch(batch_size)\n",
        "\n",
        "  iterator = tf.data.Iterator.from_structure(train_ds.output_types,train_ds.output_shapes)\n",
        "\n",
        "  train_init_op = iterator.make_initializer(train_ds)\n",
        "  val_init_op = iterator.make_initializer(val_ds)\n",
        "\n",
        "  return train_init_op, val_init_op, iterator\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKa2Fg2T7nS7"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMuKJ59NHVg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "094a5186-dbc5-4663-ba66-ac7e3360e64c"
      },
      "source": [
        "np.random.seed(9999)\n",
        "#tf.reset_default_graph()\n",
        "train_data, val_data = setup_data_paths(orig_images_path=\"./All-In-One-Image-Dehazing-Tensorflow/data/orig_images\",\n",
        "                  hazy_images_path=\"./All-In-One-Image-Dehazing-Tensorflow/data/hazy_images\");\n",
        "train_init_op, val_init_op, iterator = create_datasets(train_data,val_data,batch_size)\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "X = tf.placeholder(shape=(None,480,640,3),dtype=tf.float32)\n",
        "Y = tf.placeholder(shape=(None,480,640,3),dtype=tf.float32)\n",
        "dehazed_X = haze_net(X)\n",
        "\n",
        "loss = tf.reduce_mean(tf.square(dehazed_X-Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "trainable_variables = tf.trainable_variables()\n",
        "gradients_and_vars = optimizer.compute_gradients(loss,trainable_variables)\n",
        "clipped_gradients = [(tf.clip_by_norm(gradient,0.1),var) for gradient,var in gradients_and_vars]\n",
        "optimizer = optimizer.apply_gradients(gradients_and_vars)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-c50af54b286c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m train_data, val_data = setup_data_paths(orig_images_path=\"./All-In-One-Image-Dehazing-Tensorflow/data/orig_images\",\n\u001b[1;32m      4\u001b[0m                   hazy_images_path=\"./All-In-One-Image-Dehazing-Tensorflow/data/hazy_images\");\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_init_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_init_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IobqAjYMgsC1"
      },
      "source": [
        "\n",
        "saver = tf.train.Saver()\n",
        "load_path = None\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  with tf.Session() as sess:\n",
        "    #saver.restore(sess,'./models/model_checkpoint_' + str(5) + '.ckpt')\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "      sess.run(train_init_op)\n",
        "      batches = len(train_data) // batch_size\n",
        "      epoch_loss = 0.0\n",
        "      for batch in range(batches):\n",
        "\n",
        "        batch_data = sess.run(next_element)\n",
        "        #print(batch_data[0].shape,batch_data[1].shape)\n",
        "        #print(np.max(batch_data[0]),np.max(batch_data[1]))\n",
        "        batch_loss, _ = sess.run([loss,optimizer],feed_dict={X:batch_data[0],\n",
        "                                                            Y:batch_data[1]})\n",
        "        epoch_loss += batch_loss / float(batches)\n",
        "        if batch % 1000 == 0:\n",
        "          print(\"Training loss at batch %d: %f\\n\"%(batch,batch_loss))\n",
        "\n",
        "      train_loss = epoch_loss\n",
        "\n",
        "      sess.run(val_init_op)\n",
        "      batches= len(val_data) // batch_size\n",
        "      epoch_loss = 0.0\n",
        "      for batch in range(batches):\n",
        "        batch_data = sess.run(next_element)\n",
        "        batch_loss = sess.run(loss,feed_dict={X:batch_data[0],\n",
        "                                             Y:batch_data[1]})\n",
        "        epoch_loss += batch_loss / float(batches)\n",
        "        if batch % 100 == 0:\n",
        "          print(\"Validation loss at batch %d: %f\\n\"%(batch,batch_loss))\n",
        "          for j in range(-2 + batch_size//2):\n",
        "            x = batch_data[0][j].reshape((1,)+batch_data[0][j].shape)\n",
        "            y = batch_data[1][j].reshape((1,)+batch_data[1][j].shape)\n",
        "            dehazed_x = sess.run(dehazed_X,feed_dict={X:x,Y:y})\n",
        "            print(\"Image Number: %d\\n\"%(j))\n",
        "            showImage(x[0])\n",
        "            showImage(y[0])\n",
        "            showImage(dehazed_x[0])\n",
        "      val_loss = epoch_loss\n",
        "\n",
        "      saver.save(sess,'./models/model_checkpoint_' + str(epoch) + '.ckpt')\n",
        "\n",
        "      print(\"Epoch %d\\nTraining loss: %f\\nValidation loss: %f\\n\\n\"%(epoch,train_loss,val_loss))\n",
        "\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lCLSaQ2KbhH"
      },
      "source": [
        "next_element = iterator.get_next()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(val_init_op)\n",
        "\n",
        "  for i in range(10):\n",
        "    batch_data = sess.run(next_element)\n",
        "    for j in range(4):\n",
        "      x = batch_data[0][j].reshape((1,)+batch_data[0][j].shape)\n",
        "      showImage(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hIPF7UTJvV5"
      },
      "source": [
        "os.mkdir('./models')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlvRxPSH60vT"
      },
      "source": [
        "## **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrn7TNF_km2k"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "train_data, val_data = setup_data_paths(orig_images_path=\"./All-In-One-Image-Dehazing-Tensorflow/data/orig_images\",\n",
        "                  hazy_images_path=\"./All-In-One-Image-Dehazing-Tensorflow/data/hazy_images\");\n",
        "train_init_op, val_init_op, iterator = create_datasets(train_data,val_data,batch_size)\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "X = tf.placeholder(shape=(None,480,640,3),dtype=tf.float32)\n",
        "Y = tf.placeholder(shape=(None,480,640,3),dtype=tf.float32)\n",
        "dehazed_X = haze_net(X)\n",
        "\n",
        "loss = tf.reduce_mean(tf.square(dehazed_X-Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "trainable_variables = tf.trainable_variables()\n",
        "gradients_and_vars = optimizer.compute_gradients(loss,trainable_variables)\n",
        "clipped_gradients = [(tf.clip_by_norm(gradient,0.1),var) for gradient,var in gradients_and_vars]\n",
        "optimizer = optimizer.apply_gradients(gradients_and_vars)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZuMVtQQsUjO"
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "test_input_folder = \"./All-In-One-Image-Dehazing-Tensorflow/test_images\"\n",
        "test_output_folder = \"./All-In-One-Image-Dehazing-Tensorflow/dehazed_test_images\"\n",
        "if not os.path.exists(test_output_folder):\n",
        "  os.mkdir(test_output_folder)\n",
        "\n",
        "file_types = ['jpeg','jpg']\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  saver.restore(sess,'./All-In-One-Image-Dehazing-Tensorflow/models/model_checkpoint_7.ckpt')\n",
        "  test_image_paths = []\n",
        "  for file_type in file_types:\n",
        "    test_image_paths.extend(glob.glob(test_input_folder+\"/*.\"+file_type))\n",
        "\n",
        "\n",
        "  for path in test_image_paths:\n",
        "    image_label = path.split(test_input_folder)[-1][1:]\n",
        "    image = Image.open(path)\n",
        "    image = image.resize((640, 480))\n",
        "    image = np.asarray(image) / 255.0\n",
        "    image = image.reshape((1,) + image.shape)\n",
        "    dehazed_image = sess.run(dehazed_X,feed_dict={X:image,Y:image})\n",
        "\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,10))\n",
        "    axes[0].imshow(image[0])\n",
        "    axes[1].imshow(dehazed_image[0])\n",
        "    fig.tight_layout()\n",
        "\n",
        "    dehazed_image = np.asarray(dehazed_image[0] * 255,dtype=np.uint8)\n",
        "    mpl.image.imsave(test_output_folder + \"/\" + 'dehazed_' + image_label, dehazed_image)\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVtg2EB3_hew"
      },
      "source": [
        "os.mkdir(\"./results\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiASlDzBEHPQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}